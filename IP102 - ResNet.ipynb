{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3132677,"sourceType":"datasetVersion","datasetId":1908726}],"dockerImageVersionId":30476,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\n# ------------------ Configs ------------------ #\ndata_dir = '/kaggle/input/ip02-dataset/classification/'  # should contain train/, val/, test/ folders\nbatch_size = 64\nlearning_rate = 0.01\nnum_epochs = 30\nlr_step_size = 40\nlr_gamma = 0.1\nmomentum = 0.9\nweight_decay = 0.0005\ndropout_rate = 0.3\ninput_size = 224\nnum_workers = 4\n# num_classes will be determined from the dataset\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ------------------ Data Transforms ------------------ #\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((input_size, input_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((input_size, input_size)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((input_size, input_size)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n}\n\n# ------------------ Datasets & Dataloaders ------------------ #\nimage_datasets = {\n    x: datasets.ImageFolder(os.path.join(data_dir, x), transform=data_transforms[x])\n    for x in ['train', 'val', 'test']\n}\n\nnum_classes = len(image_datasets['train'].classes)\nprint(\"âœ… num_classes set to:\", num_classes)\n\ndataloaders = {\n    x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=(x != 'test'), num_workers=num_workers)\n    for x in ['train', 'val', 'test']\n}\n\nclass_names = image_datasets['train'].classes\n\n# ------------------ Model Setup ------------------ #\nmodel = models.resnet50(pretrained=True)\nin_features = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Dropout(p=dropout_rate),\n    nn.Linear(in_features, num_classes)\n)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate,\n                      momentum=momentum, weight_decay=weight_decay)\nscheduler = StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n\n# ------------------ Training Loop ------------------ #\ndef train_model():\n    best_val_acc = 0.0\n    best_model_path = 'best_resnet50_model.pth'\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print('-' * 20)\n\n        for phase in ['train', 'val']:\n            model.train() if phase == 'train' else model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(image_datasets[phase])\n            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n\n            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n\n            # ğŸ”½ Save best model on validation\n            if phase == 'val' and epoch_acc > best_val_acc:\n                best_val_acc = epoch_acc\n                torch.save(model.state_dict(), best_model_path)\n                print(f\"âœ… Best model saved at epoch {epoch+1} with val acc: {epoch_acc:.4f}\")\n\n        scheduler.step()\n\n    print(f\"\\nğŸ Training completed. Best Val Acc: {best_val_acc:.4f}\")\n\n\n# ------------------ Evaluation on Test Set ------------------ #\ndef evaluate_model():\n    model.eval()\n    all_preds = []\n    all_labels = []\n    top1_correct = 0\n    top5_correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloaders['test'], desc=\"Evaluating\"):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, top1_preds = outputs.topk(1, dim=1)\n            top5_preds = outputs.topk(5, dim=1).indices\n\n            all_preds.extend(top1_preds.squeeze().cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            top1_correct += (top1_preds.squeeze() == labels).sum().item()\n\n            for i in range(labels.size(0)):\n                if labels[i] in top5_preds[i]:\n                    top5_correct += 1\n\n            total += labels.size(0)\n\n    top1_acc = top1_correct / total\n    top5_acc = top5_correct / total\n\n    print(f\"\\nâœ… Top-1 Accuracy: {top1_acc:.4f}\")\n    print(f\"âœ… Top-5 Accuracy: {top5_acc:.4f}\\n\")\n\n    print(\"ğŸ“Š Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n\n    print(\"ğŸ§¾ Confusion Matrix:\")\n    print(confusion_matrix(all_labels, all_preds))\n\n# ------------------ Run ------------------ #\nif __name__ == \"__main__\":\n    train_model()\n    evaluate_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T16:15:49.404399Z","iopub.execute_input":"2025-06-15T16:15:49.404778Z","iopub.status.idle":"2025-06-15T18:25:36.763529Z","shell.execute_reply.started":"2025-06-15T16:15:49.404747Z","shell.execute_reply":"2025-06-15T18:25:36.762281Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"âœ… num_classes set to: 102\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 269MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n--------------------\ntrain Loss: 2.2041 Acc: 0.4515\nval Loss: 1.8279 Acc: 0.5197\nâœ… Best model saved at epoch 1 with val acc: 0.5197\nEpoch 2/30\n--------------------\ntrain Loss: 1.5644 Acc: 0.5749\nval Loss: 1.6121 Acc: 0.5619\nâœ… Best model saved at epoch 2 with val acc: 0.5619\nEpoch 3/30\n--------------------\ntrain Loss: 1.3391 Acc: 0.6270\nval Loss: 1.5899 Acc: 0.5790\nâœ… Best model saved at epoch 3 with val acc: 0.5790\nEpoch 4/30\n--------------------\ntrain Loss: 1.1778 Acc: 0.6635\nval Loss: 1.5275 Acc: 0.5903\nâœ… Best model saved at epoch 4 with val acc: 0.5903\nEpoch 5/30\n--------------------\ntrain Loss: 1.0612 Acc: 0.6896\nval Loss: 1.6234 Acc: 0.5707\nEpoch 6/30\n--------------------\ntrain Loss: 0.9688 Acc: 0.7143\nval Loss: 1.7634 Acc: 0.5478\nEpoch 7/30\n--------------------\ntrain Loss: 0.8783 Acc: 0.7392\nval Loss: 1.5081 Acc: 0.6067\nâœ… Best model saved at epoch 7 with val acc: 0.6067\nEpoch 8/30\n--------------------\ntrain Loss: 0.8270 Acc: 0.7518\nval Loss: 1.6930 Acc: 0.5775\nEpoch 9/30\n--------------------\ntrain Loss: 0.7706 Acc: 0.7678\nval Loss: 1.6313 Acc: 0.5908\nEpoch 10/30\n--------------------\ntrain Loss: 0.7188 Acc: 0.7813\nval Loss: 1.6613 Acc: 0.5850\nEpoch 11/30\n--------------------\ntrain Loss: 0.6722 Acc: 0.7959\nval Loss: 1.5934 Acc: 0.6062\nEpoch 12/30\n--------------------\ntrain Loss: 0.6132 Acc: 0.8121\nval Loss: 1.6411 Acc: 0.5938\nEpoch 13/30\n--------------------\ntrain Loss: 0.5766 Acc: 0.8223\nval Loss: 1.7265 Acc: 0.5854\nEpoch 14/30\n--------------------\ntrain Loss: 0.5492 Acc: 0.8319\nval Loss: 1.7166 Acc: 0.6002\nEpoch 15/30\n--------------------\ntrain Loss: 0.4944 Acc: 0.8479\nval Loss: 1.7888 Acc: 0.5840\nEpoch 16/30\n--------------------\ntrain Loss: 0.4829 Acc: 0.8531\nval Loss: 1.7721 Acc: 0.5779\nEpoch 17/30\n--------------------\ntrain Loss: 0.4620 Acc: 0.8579\nval Loss: 1.7131 Acc: 0.6015\nEpoch 18/30\n--------------------\ntrain Loss: 0.4372 Acc: 0.8674\nval Loss: 1.9519 Acc: 0.5602\nEpoch 19/30\n--------------------\ntrain Loss: 0.4185 Acc: 0.8726\nval Loss: 1.8517 Acc: 0.5868\nEpoch 20/30\n--------------------\ntrain Loss: 0.3986 Acc: 0.8769\nval Loss: 1.8213 Acc: 0.5879\nEpoch 21/30\n--------------------\ntrain Loss: 0.3875 Acc: 0.8798\nval Loss: 1.7587 Acc: 0.5920\nEpoch 22/30\n--------------------\ntrain Loss: 0.3715 Acc: 0.8876\nval Loss: 1.7371 Acc: 0.6060\nEpoch 23/30\n--------------------\ntrain Loss: 0.3581 Acc: 0.8909\nval Loss: 1.9444 Acc: 0.5651\nEpoch 24/30\n--------------------\ntrain Loss: 0.3433 Acc: 0.8958\nval Loss: 1.8564 Acc: 0.5918\nEpoch 25/30\n--------------------\ntrain Loss: 0.3452 Acc: 0.8960\nval Loss: 1.8987 Acc: 0.5870\nEpoch 26/30\n--------------------\ntrain Loss: 0.3155 Acc: 0.9043\nval Loss: 1.8197 Acc: 0.5998\nEpoch 27/30\n--------------------\ntrain Loss: 0.3097 Acc: 0.9066\nval Loss: 1.8275 Acc: 0.5956\nEpoch 28/30\n--------------------\ntrain Loss: 0.2981 Acc: 0.9102\nval Loss: 1.9642 Acc: 0.5746\nEpoch 29/30\n--------------------\ntrain Loss: 0.2949 Acc: 0.9126\nval Loss: 1.9316 Acc: 0.5882\nEpoch 30/30\n--------------------\ntrain Loss: 0.2887 Acc: 0.9129\nval Loss: 1.9038 Acc: 0.5871\n\nğŸ Training completed. Best Val Acc: 0.6067\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 354/354 [01:22<00:00,  4.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Top-1 Accuracy: 0.5803\nâœ… Top-5 Accuracy: 0.8188\n\nğŸ“Š Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.62      0.66      0.64       335\n           1       0.27      0.32      0.30       147\n          10       0.53      0.64      0.58       257\n         100       0.48      0.56      0.52       138\n         101       0.79      0.72      0.75      1723\n          11       0.26      0.42      0.32       122\n          12       0.79      0.42      0.55        52\n          13       0.25      0.37      0.30       123\n          14       0.78      0.73      0.75       258\n          15       0.97      0.84      0.90       495\n          16       0.63      0.68      0.65       267\n          17       0.52      0.33      0.41        45\n          18       0.47      0.39      0.42       257\n          19       0.30      0.19      0.23       148\n           2       0.34      0.30      0.32        79\n          20       0.41      0.22      0.28       144\n          21       0.54      0.53      0.54       159\n          22       0.41      0.70      0.52       510\n          23       0.30      0.50      0.37       322\n          24       0.57      0.68      0.62      1229\n          25       0.78      0.75      0.77       170\n          26       0.64      0.72      0.68       208\n          27       0.39      0.25      0.31       197\n          28       0.14      0.22      0.17        98\n          29       0.30      0.25      0.28       213\n           3       0.51      0.50      0.50       316\n          30       0.66      0.75      0.70       146\n          31       0.83      0.29      0.43        70\n          32       0.53      0.23      0.32        74\n          33       0.43      0.57      0.49        88\n          34       0.38      0.65      0.48       102\n          35       0.59      0.24      0.34        42\n          36       0.50      0.19      0.27        32\n          37       0.71      0.65      0.68       237\n          38       0.44      0.25      0.32       257\n          39       0.50      0.45      0.47       482\n           4       0.61      0.45      0.52       152\n          40       0.65      0.41      0.50        90\n          41       0.81      0.43      0.56        81\n          42       0.71      0.22      0.33        93\n          43       0.62      0.60      0.61        58\n          44       0.48      0.30      0.37       158\n          45       0.23      0.29      0.26       320\n          46       0.37      0.35      0.36       197\n          47       0.39      0.51      0.44       246\n          48       0.73      0.75      0.74       419\n          49       0.46      0.49      0.48       197\n           5       0.64      0.62      0.63       152\n          50       0.68      0.60      0.64       423\n          51       0.60      0.44      0.51       570\n          52       0.19      0.13      0.15        78\n          53       0.61      0.43      0.50        54\n          54       0.29      0.26      0.27       264\n          55       0.21      0.29      0.24        56\n          56       0.93      0.69      0.79       141\n          57       0.39      0.24      0.30       115\n          58       0.82      0.78      0.80       421\n          59       0.61      0.33      0.43       107\n           6       0.30      0.58      0.39       111\n          60       0.67      0.30      0.42        53\n          61       0.92      0.46      0.61        24\n          62       0.97      0.79      0.87        85\n          63       0.71      0.19      0.30        26\n          64       0.77      0.29      0.42        93\n          65       0.24      0.69      0.35        42\n          66       0.92      0.53      0.68       230\n          67       0.69      0.84      0.76      1594\n          68       0.69      0.76      0.72       345\n          69       0.65      0.77      0.70       384\n           7       0.48      0.32      0.38       251\n          70       0.80      0.62      0.70      1525\n          71       0.67      0.65      0.66       208\n          72       0.69      0.41      0.51        22\n          73       0.79      0.89      0.84       135\n          74       0.74      0.45      0.56       116\n          75       0.63      0.63      0.63        52\n          76       0.50      0.75      0.60       217\n          77       0.62      0.59      0.60       126\n          78       0.56      0.69      0.62        78\n          79       0.55      0.60      0.58        68\n           8       0.44      0.47      0.46       268\n          80       0.76      0.57      0.65        23\n          81       0.23      0.50      0.32        30\n          82       0.72      0.88      0.80       208\n          83       0.56      0.73      0.63       117\n          84       0.31      0.74      0.44       132\n          85       0.30      0.22      0.25        51\n          86       0.70      0.38      0.50       392\n          87       0.61      0.54      0.58        94\n          88       0.83      0.45      0.59       122\n          89       0.39      0.19      0.26        57\n           9       0.27      0.38      0.32       166\n          90       0.52      0.38      0.44        68\n          91       0.33      0.59      0.42       106\n          92       0.50      0.65      0.56       243\n          93       0.56      0.30      0.39       152\n          94       0.79      0.55      0.65       174\n          95       0.79      0.57      0.66       147\n          96       0.69      0.14      0.24        63\n          97       0.62      0.37      0.46        92\n          98       0.47      0.32      0.38        28\n          99       0.43      0.54      0.48       167\n\n    accuracy                           0.58     22619\n   macro avg       0.56      0.49      0.50     22619\nweighted avg       0.61      0.58      0.58     22619\n\nğŸ§¾ Confusion Matrix:\n[[220  17   3 ...   0   0   0]\n [ 27  47   4 ...   1   0   1]\n [  6   3 165 ...   0   0   2]\n ...\n [  0   1   0 ...  34   0   0]\n [  0   0   0 ...   0   9   0]\n [  0   0   7 ...   0   0  90]]\n","output_type":"stream"}],"execution_count":1}]}